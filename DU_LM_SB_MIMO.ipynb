{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16b11933",
   "metadata": {},
   "source": [
    "    这段代码主要用于设置一个深度学习模型的基础参数，包括天线数量、训练参数、评估参数以及SB算法的特定参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11802a02-7d7a-4109-adb0-96218daf71b6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11802a02-7d7a-4109-adb0-96218daf71b6",
    "outputId": "fec8c134-3c7b-44eb-905f-558978cdaee9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import math  # 提供基本的数学运算\n",
    "import random  # 用于生成随机数\n",
    "import numpy as np  # 用于数值计算\n",
    "import copy  # 深拷贝对象\n",
    "import matplotlib.pyplot as plt  # 用于数据可视化\n",
    "import torch  # 导入PyTorch库，用于深度学习\n",
    "import torch.nn as nn  # 导入PyTorch的神经网络模块\n",
    "import torch.optim as optim  # 导入PyTorch的优化器模块\n",
    "import torch.nn.functional as F  # 导入PyTorch的功能性模块，包含多种神经网络功能\n",
    "from scipy.linalg import toeplitz  # 从SciPy库导入toeplitz函数，用于生成Toeplitz矩阵\n",
    "import logging\n",
    "\n",
    "## 配置日志记录器\n",
    "\n",
    "# 创建日志记录器\n",
    "logger = logging.getLogger('my_logger')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# 创建文件处理器\n",
    "file_handler = logging.FileHandler('24_12_01.log')\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "# 创建控制台处理器\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "# 创建日志格式化器\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "# 将格式化器添加到处理器\n",
    "file_handler.setFormatter(formatter)\n",
    "console_handler.setFormatter(formatter)\n",
    "# 将处理器添加到日志记录器\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# 设置计算设备\n",
    "device = torch.device('cuda') # 'cpu' or 'cuda'\n",
    "print(torch.__version__)  # 打印PyTorch的版本\n",
    "\n",
    "\n",
    "# 模型参数\n",
    "n=16 # 发射天线数量\n",
    "m=16 # 接收天线数量\n",
    "##\n",
    "N=2*n  # 计算总发射天线数量\n",
    "M=2*m  # 计算总接收天线数量\n",
    "\n",
    "# 训练参数\n",
    "max_itr = 10 # 最大迭代次数\n",
    "bs = 2000 # 每个小批量的样本大小，即每次训练使用的样本数量\n",
    "num_batch = 1000 # 小批量的数量，即总的训练批次\n",
    "lr_adam = 5e-3 # Adam 优化器的学习率\n",
    "##\n",
    "\n",
    "# 评估泛化误差的参数\n",
    "#total_itr=30 # total number of iterations (multiple number of \"itr\")  ## 总迭代次数（多个“itr”的乘积）\n",
    "sample = 1000  # 评估时的样本数量\n",
    "bs_ = 2000  # 评估时的小批量大小\n",
    "##\n",
    "\n",
    "\n",
    "# SB 设置\n",
    "eps = 1.0 # 控制某种阈值或精度的参数，通常用于算法收敛\n",
    "T_max = max_itr # 最大迭代次数，与 max_itr 相同\n",
    "pump_SB = 1.0/(T_max*eps) # 泵系数，计算公式为 1.0 / (T_max * eps)\n",
    "D_SB = 1. # delta参数，通常用于控制学习率或收敛速度（步长参数）\n",
    "xi_SB = 0.1 # 初始xi值，可能用于某种动态调整（停止条件或更新策略）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd09e30",
   "metadata": {},
   "source": [
    "这段代码定义了一个基于深度展开技术的信号检测器，用于处理无线通信中的信号恢复问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "504e28c8-a249-40c2-96a5-6b7ef03c186f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "504e28c8-a249-40c2-96a5-6b7ef03c186f",
    "outputId": "87593ac1-463d-4cf1-c360-b20b6ac383e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#_  n= 16 m= 16 max_itr= 50 bs= 2000 num_batch= 1000 learning_rate= 0.005\n",
      "-------------------\n",
      "snr= tensor(5)\n",
      "loss0:0.7003787755966187 BER: tensor(0.1760, device='cuda:0')\n",
      "loss100:0.6888607740402222 BER: tensor(0.1738, device='cuda:0')\n",
      "loss200:0.6930116415023804 BER: tensor(0.1750, device='cuda:0')\n",
      "loss300:0.6507097482681274 BER: tensor(0.1646, device='cuda:0')\n",
      "loss400:0.63661789894104 BER: tensor(0.1611, device='cuda:0')\n",
      "loss500:0.6091437339782715 BER: tensor(0.1540, device='cuda:0')\n",
      "loss600:0.6838785409927368 BER: tensor(0.1726, device='cuda:0')\n",
      "loss700:0.6755576133728027 BER: tensor(0.1701, device='cuda:0')\n",
      "loss800:0.6998711228370667 BER: tensor(0.1762, device='cuda:0')\n",
      "loss900:0.6735630035400391 BER: tensor(0.1695, device='cuda:0')\n",
      "-----\n",
      " SNR: 5\n",
      "eps= Parameter containing:\n",
      "tensor([1.2484, 1.2858, 0.9469, 1.1548, 0.9421, 0.7760, 1.2299, 0.8947, 1.0993,\n",
      "        1.3356, 0.9182, 0.9377, 1.4067, 0.7225, 0.8292, 1.3221, 1.0235, 0.9419,\n",
      "        1.4237, 1.1222, 0.8795, 1.2661, 1.1246, 1.0210, 1.0643, 1.2787, 1.3584,\n",
      "        1.0016, 0.9749, 0.9544, 1.1884, 1.3534, 1.2233, 1.1621, 1.1908, 1.2466,\n",
      "        1.2254, 1.2151, 1.2344, 1.2774, 1.2739, 1.2875, 1.2561, 1.2293, 1.1842,\n",
      "        1.1595, 1.1534, 1.1537, 1.1440, 2.0758], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "beta= Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "xi= Parameter containing:\n",
      "tensor([1.1493], device='cuda:0', requires_grad=True)\n",
      "lam= tensor([1.2871], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "train done\n",
      "SNR: 5 BER (generalization): 0.17059630155563354\n",
      "-------------------\n",
      "snr= tensor(10)\n",
      "loss0:0.17804810404777527 BER: tensor(0.0453, device='cuda:0')\n",
      "loss100:0.16139861941337585 BER: tensor(0.0413, device='cuda:0')\n",
      "loss200:0.1688634157180786 BER: tensor(0.0433, device='cuda:0')\n",
      "loss300:0.11968202888965607 BER: tensor(0.0309, device='cuda:0')\n",
      "loss400:0.1423417329788208 BER: tensor(0.0364, device='cuda:0')\n",
      "loss500:0.10664928704500198 BER: tensor(0.0277, device='cuda:0')\n",
      "loss600:0.15741059184074402 BER: tensor(0.0405, device='cuda:0')\n",
      "loss700:0.15134620666503906 BER: tensor(0.0389, device='cuda:0')\n",
      "loss800:0.137552410364151 BER: tensor(0.0355, device='cuda:0')\n",
      "loss900:0.1430594027042389 BER: tensor(0.0369, device='cuda:0')\n",
      "-----\n",
      " SNR: 10\n",
      "eps= Parameter containing:\n",
      "tensor([0.9699, 1.0481, 0.8942, 0.8906, 1.0002, 0.6826, 0.7066, 0.7240, 0.8125,\n",
      "        0.7838, 0.7421, 0.7040, 0.7573, 0.9161, 0.7899, 0.7452, 1.0189, 0.9462,\n",
      "        0.8792, 1.0187, 0.9258, 0.9370, 1.1774, 1.1301, 0.7793, 0.8872, 1.0081,\n",
      "        0.9498, 0.8866, 0.8775, 0.8926, 0.7722, 0.8274, 0.7618, 0.7500, 0.7259,\n",
      "        0.7360, 0.7149, 0.6542, 0.6400, 0.6368, 0.6344, 0.5979, 0.5839, 0.5624,\n",
      "        0.5820, 0.5919, 0.6142, 0.5936, 1.4158], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "beta= Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "xi= Parameter containing:\n",
      "tensor([0.9910], device='cuda:0', requires_grad=True)\n",
      "lam= tensor([1.6093], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "train done\n",
      "SNR: 10 BER (generalization): 0.036536794155836105\n",
      "-------------------\n",
      "snr= tensor(15)\n",
      "loss0:0.005135321989655495 BER: tensor(0.0013, device='cuda:0')\n",
      "loss100:0.0006228672573342919 BER: tensor(0.0002, device='cuda:0')\n",
      "loss200:0.0015470313373953104 BER: tensor(0.0004, device='cuda:0')\n",
      "loss300:0.00037324934964999557 BER: tensor(0.0001, device='cuda:0')\n",
      "loss400:0.0007884196238592267 BER: tensor(0.0002, device='cuda:0')\n",
      "loss500:0.00012851700012106448 BER: tensor(4.6875e-05, device='cuda:0')\n",
      "loss600:0.00030583023908548057 BER: tensor(6.2500e-05, device='cuda:0')\n",
      "loss700:0.0007201397675089538 BER: tensor(0.0002, device='cuda:0')\n",
      "loss800:0.00014301552437245846 BER: tensor(3.1250e-05, device='cuda:0')\n",
      "loss900:0.0004892548313364387 BER: tensor(0.0001, device='cuda:0')\n",
      "-----\n",
      " SNR: 15\n",
      "eps= Parameter containing:\n",
      "tensor([0.9382, 1.0199, 0.9435, 0.7774, 1.0236, 0.9324, 0.8716, 1.0464, 1.1018,\n",
      "        0.8626, 1.0611, 1.1084, 0.9023, 1.0678, 0.8449, 1.0413, 0.9788, 0.8896,\n",
      "        0.7852, 0.9455, 0.7973, 0.8035, 0.9300, 1.1195, 1.0374, 0.9944, 0.9778,\n",
      "        1.0092, 0.9590, 0.8601, 0.8977, 0.9531, 1.0025, 1.0182, 1.0488, 1.0001,\n",
      "        0.9660, 0.9454, 0.9459, 0.9476, 0.9152, 0.8821, 0.8234, 0.8058, 0.7905,\n",
      "        0.8449, 0.8823, 0.9163, 0.9190, 1.1730], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "beta= Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "xi= Parameter containing:\n",
      "tensor([1.0682], device='cuda:0', requires_grad=True)\n",
      "lam= tensor([3.0742], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "train done\n",
      "SNR: 15 BER (generalization): 0.00013482813665177673\n",
      "-------------------\n",
      "snr= tensor(20)\n",
      "loss0:1.5021687005400963e-08 BER: tensor(0., device='cuda:0')\n",
      "loss100:3.328868569685284e-10 BER: tensor(0., device='cuda:0')\n",
      "loss200:9.713287951740313e-10 BER: tensor(0., device='cuda:0')\n",
      "loss300:1.4640966217172036e-10 BER: tensor(0., device='cuda:0')\n",
      "loss400:1.6878566599931588e-10 BER: tensor(0., device='cuda:0')\n",
      "loss500:1.0013168072475764e-10 BER: tensor(0., device='cuda:0')\n",
      "loss600:9.344152188117683e-12 BER: tensor(0., device='cuda:0')\n",
      "loss700:9.982080717563235e-10 BER: tensor(0., device='cuda:0')\n",
      "loss800:6.9596082369338674e-12 BER: tensor(0., device='cuda:0')\n",
      "loss900:5.4523791731542204e-11 BER: tensor(0., device='cuda:0')\n",
      "-----\n",
      " SNR: 20\n",
      "eps= Parameter containing:\n",
      "tensor([1.0595, 1.0473, 1.1453, 1.0138, 1.1777, 1.1450, 1.0514, 1.0911, 1.1184,\n",
      "        0.9907, 1.0055, 1.1099, 1.0765, 0.9634, 1.0331, 1.1083, 1.0441, 1.0099,\n",
      "        1.1977, 1.1943, 1.0719, 1.1211, 0.9893, 0.9825, 1.0366, 1.0220, 0.9890,\n",
      "        1.0162, 1.0417, 1.0695, 1.0445, 0.9959, 1.0006, 1.0341, 1.1264, 1.0791,\n",
      "        1.0080, 0.9822, 1.0035, 1.0546, 1.0834, 1.1012, 1.0691, 1.0519, 1.0270,\n",
      "        1.0525, 1.0488, 1.0752, 0.9792, 1.2080], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "beta= Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "xi= Parameter containing:\n",
      "tensor([1.1179], device='cuda:0', requires_grad=True)\n",
      "lam= tensor([1.8026], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "train done\n",
      "SNR: 20 BER (generalization): 5.625000198961061e-07\n",
      "-------------------\n",
      "snr= tensor(25)\n",
      "loss0:1.709369278068973e-11 BER: tensor(0., device='cuda:0')\n",
      "loss100:1.595661428509254e-12 BER: tensor(0., device='cuda:0')\n",
      "loss200:6.606779359707993e-12 BER: tensor(0., device='cuda:0')\n",
      "loss300:1.290814243687377e-13 BER: tensor(0., device='cuda:0')\n",
      "loss400:1.423818880722308e-13 BER: tensor(0., device='cuda:0')\n",
      "loss500:7.683796090715567e-13 BER: tensor(0., device='cuda:0')\n",
      "loss600:9.677170496142848e-14 BER: tensor(0., device='cuda:0')\n",
      "loss700:1.2400501632481298e-12 BER: tensor(0., device='cuda:0')\n",
      "loss800:5.953573680057833e-13 BER: tensor(0., device='cuda:0')\n",
      "loss900:5.687954989820609e-13 BER: tensor(0., device='cuda:0')\n",
      "-----\n",
      " SNR: 25\n",
      "eps= Parameter containing:\n",
      "tensor([1.0150, 1.0208, 1.0041, 0.9846, 1.0114, 0.9955, 1.0090, 0.9996, 0.9881,\n",
      "        1.0011, 0.9971, 0.9989, 1.0067, 1.0013, 0.9949, 0.9879, 0.9992, 1.0159,\n",
      "        0.9861, 1.0205, 0.9831, 0.9900, 0.9907, 0.9843, 0.9991, 1.0067, 0.9992,\n",
      "        1.0054, 1.0168, 1.0009, 1.0007, 0.9976, 1.0211, 1.0025, 1.0251, 0.9873,\n",
      "        0.9965, 0.9941, 0.9905, 0.9819, 0.9869, 1.0044, 1.0048, 1.0090, 0.9958,\n",
      "        1.0123, 1.0087, 1.0241, 0.9455, 1.1103], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "beta= Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "xi= Parameter containing:\n",
      "tensor([1.1174], device='cuda:0', requires_grad=True)\n",
      "lam= tensor([0.9380], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "train done\n",
      "SNR: 25 BER (generalization): 0.0\n",
      "-------------------\n",
      "snr= tensor(30)\n",
      "loss0:9.15547477241263e-13 BER: tensor(0., device='cuda:0')\n",
      "loss100:2.092801572230879e-13 BER: tensor(0., device='cuda:0')\n",
      "loss200:1.0118202803388598e-10 BER: tensor(0., device='cuda:0')\n",
      "loss300:1.9589989352919446e-12 BER: tensor(0., device='cuda:0')\n",
      "loss400:2.184060874863003e-12 BER: tensor(0., device='cuda:0')\n",
      "loss500:1.1136561291202884e-11 BER: tensor(0., device='cuda:0')\n",
      "loss600:1.9787704461093902e-12 BER: tensor(0., device='cuda:0')\n",
      "loss700:5.91240684882921e-12 BER: tensor(0., device='cuda:0')\n",
      "loss800:1.4876416071230025e-11 BER: tensor(0., device='cuda:0')\n",
      "loss900:6.2967638915500945e-12 BER: tensor(0., device='cuda:0')\n",
      "-----\n",
      " SNR: 30\n",
      "eps= Parameter containing:\n",
      "tensor([1.0692, 1.0750, 0.9503, 0.9309, 0.9576, 0.9417, 0.9552, 1.0534, 0.9343,\n",
      "        0.9473, 0.9434, 0.9454, 0.9530, 0.9475, 0.9414, 1.0419, 1.0526, 0.9623,\n",
      "        0.9324, 0.9669, 0.9315, 0.9364, 0.9372, 0.9319, 0.9456, 0.9529, 0.9456,\n",
      "        1.0539, 0.9634, 0.9473, 0.9471, 0.9442, 0.9678, 0.9489, 0.9715, 0.9337,\n",
      "        0.9428, 0.9405, 0.9368, 0.9283, 0.9333, 0.9507, 0.9511, 0.9554, 0.9423,\n",
      "        0.9585, 0.9569, 0.9550, 0.9481, 0.9967], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "beta= Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "xi= Parameter containing:\n",
      "tensor([1.0927], device='cuda:0', requires_grad=True)\n",
      "lam= tensor([1.0411], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "train done\n",
      "SNR: 30 BER (generalization): 1.4062501918488124e-07\n"
     ]
    }
   ],
   "source": [
    "# 生成随机输入信号x\n",
    "def x_gen(bs,n):  # 参数 bs(int):批量大小 n(int):天线数量 返回Tensor: 生成的信号x\n",
    "    # torch.rand(bs,n) 会创建一个形状为 (bs, n) 的张量（即一个包含 bs 行和 n 列的矩阵），其中的元素是从标准均匀分布（在区间 [0, 1) 上）随机采样的浮点数。\n",
    "    # .to(device) 是一个方法，它将生成的张量 x 转移到指定的设备上。这里的 device 可以是CPU或GPU。\n",
    "    # 如果 device 是一个GPU设备，这个操作会将张量 x 从CPU内存复制到GPU内存中，以便后续的计算可以在GPU上执行，从而利用GPU的并行处理能力加速计算。\n",
    "    x = torch.rand(bs,n).to(device)  \n",
    "    x[x<0.5] = -1  # 将小于0.5的值设为-1\n",
    "    x[x>0.5] = 1  # 将大于0.5的值设为1\n",
    "    return x\n",
    "\n",
    "# 生成带有噪声的接收信号y\n",
    "# 参数 bs (int): 批量大小 m (int): 接收天线数量  x0 (Tensor): 发送信号 \n",
    "# H (Tensor): 信道矩阵 sigma_std (float): 噪声标准差\n",
    "# 返回Tensor: 生成的接收信号y\n",
    "def y_gen(bs,m,x0,H,sigma_std):  \n",
    "    # 模拟一个信号通过一个信道的过程，并添加高斯噪声\n",
    "    return x0@H+ torch.normal(0.0, sigma_std*torch.ones(bs, m)).to(device)\n",
    "    # x0 是一个张量，代表原始信号。\n",
    "    # H 是信道矩阵，代表信号传输过程中的信道效应。\n",
    "    # @ 是PyTorch中的矩阵乘法运算符。\n",
    "    # sigma_std 是根据信噪比（SNR）计算得到的标准差，它控制噪声的强度。\n",
    "    # torch.ones(bs, m) 生成一个形状为 (bs, m) 的张量，其中的元素都是 1。\n",
    "    # sigma_std * torch.ones(bs, m) 将每个元素的标准差乘以 sigma_std，生成一个标准差与信噪比相关的噪声张量。\n",
    "\n",
    "# 将H和y转换成QUBO形式\n",
    "def trans_2_QUBO(H,y):\n",
    "    J = H@H.t() - torch.diag(torch.diagonal(H@H.t(),0))  # 构建二次项的耦合强度矩阵 J\n",
    "    # H.t() 是 H 的转置。\n",
    "    # torch.diagonal(H @ H.t(), 0) 从 H @ H.t() 的结果中提取主对角线上的元素（即 diagonal=0 表示主对角线）。\n",
    "    # torch.diag(torch.diagonal(H @ H.t(), 0)) 将提取的对角元素重新构造成一个对角矩阵。\n",
    "    h = -2*y@H.t()  # 计算线性项\n",
    "    # y @ H.t() 计算观测信号 y 和信道矩阵转置 H.t() 的矩阵乘积，结果是一个向量。\n",
    "    # 这个缩放因子 -2 是构建QUBO问题时的常见选择，用于调整目标函数中线性项的系数。\n",
    "    # 结果 h 是一个向量，其中的每个元素是观测信号 y 通过信道矩阵 H 后的线性变换结果，乘以 -2 后得到的偏置项系数。\n",
    "    lmax_2 = ((J*J).sum()/(N*(N-1)))**0.5 # 估计耦合强度矩阵 J 的最大特征值\n",
    "    # sum() 函数计算矩阵 J * J 中所有元素的总和。\n",
    "    # N 是变量的数量，即矩阵 J 的大小。N * (N - 1) 是矩阵 J 中非对角元素的总数。将总和除以 N * (N - 1) 进行归一化，得到 J 矩阵元素平方的平均值。\n",
    "    # 开平方根 ** 0.5\n",
    "    return J,h, 1.0/(2*N**0.5*lmax_2)  # 返回三个值：J，h，以及一个归一化因子。\n",
    "    # 2 * N ** 0.5 是一个缩放因子，它考虑了变量数量的平方根。有助于平衡目标函数中的不同项。\n",
    "\n",
    "# LMMSE下的QUBO转换\n",
    "def trans_2_QUBO_LMMSE(H,y,lam):  # 参数 lam (Tensor): 正则化参数\n",
    "    H_inv = torch.linalg.inv(H.t()@H+lam*torch.eye(M,device=device)) # 计算矩阵H^TH+λI的逆\n",
    "    # torch.eye(M, device=device) 创建一个大小为M*M的单位矩阵，其中M是H的列数。\n",
    "    # lam 是正则化参数，用于控制正则化项的强度。\n",
    "    # torch.linalg.inv() 计算上述矩阵的逆。\n",
    "    J = H@H_inv@H.t() - torch.diag(torch.diagonal(H@H_inv@H.t(),0))\n",
    "    h = -2*y@H_inv@H.t()\n",
    "    lmax_2 = ((J*J).sum()/(N*(N-1)))**0.5 #estimated max. eig.\n",
    "    return J,h, 1.0/(2*N**0.5*lmax_2)\n",
    "\n",
    "def BER(x,y):\n",
    "    z = torch.ones(x.size()).to(device)\n",
    "    z[torch.isclose(torch.sign(x),torch.sign(y))] = 0.\n",
    "    return z.sum()/(z.numel())\n",
    "\n",
    "seed_ =12\n",
    "torch.manual_seed(seed_)\n",
    "\n",
    "# QPSK\n",
    "def H_gen(m,n):\n",
    "    H_re = torch.normal(0.0, std=math.sqrt(0.5) * torch.ones(n,m))\n",
    "    H_im = torch.normal(0.0, std=math.sqrt(0.5) * torch.ones(n,m))  # sensing matrix\n",
    "    H = torch.cat((torch.cat((H_re,H_im),0),torch.cat((-1*H_im,H_re),0)),1)\n",
    "    H = H.to(device)\n",
    "    return H\n",
    "\n",
    "\n",
    "def est_SNR(snr,m,n):\n",
    "    sigma2 = (2*n/math.pow(10,snr/10.0))/2.0\n",
    "    sigma_std = math.sqrt(sigma2)\n",
    "    return sigma_std\n",
    "\n",
    "### DU\n",
    "class DU_dSB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DU_dSB,self).__init__()\n",
    "        self.eps = nn.Parameter( eps + 0.01 * torch.randn(max_itr,device=device))\n",
    "        self.lam = nn.Parameter(torch.ones(1,device=device))\n",
    "        self.xi = nn.Parameter(torch.ones(1,device=device))\n",
    "        self.beta = nn.Parameter(1.0*torch.ones(max_itr,device=device))\n",
    "\n",
    "    def Pump(self,t):\n",
    "        tsum = (self.eps).sum()\n",
    "        return t/tsum\n",
    "\n",
    "    def Softclamp(self,x):\n",
    "        si = nn.SiLU()\n",
    "        return si(10*(x+1))/10-si(10*(x-1))/10 -1\n",
    "\n",
    "    def dSB(self,k, q,p,t,eps,J,h,D_SB,pump_SB,xi_SB):\n",
    "        sq = q\n",
    "        DE_QUBO = 0.5 * h + sq@J\n",
    "        p_ = p - eps * ((D_SB-self.Pump(t)) * q + self.xi*xi_SB * DE_QUBO) #diff(q,Po)\n",
    "        q_ = q + (eps * D_SB) * p_ #diff(p,K)\n",
    "        q_2 = self.Softclamp(q_) #torch.clamp(q_, min=-1., max=1.)\n",
    "        p_ = p_ - p_ * torch.sigmoid(100* (torch.abs(q_)-1.01) )\n",
    "        # naively torch.heaviside(1-torch.abs(q_), torch.zeros(1,device=device)) but its derivative is unaveilabe\n",
    "        return q_2,p_\n",
    "\n",
    "    def forward(self, H,y,itr,bs):\n",
    "        J, h, xi_SB= trans_2_QUBO_LMMSE(H,y,self.lam**2)\n",
    "        q = torch.zeros(bs, N,device=device) # x\n",
    "        p = torch.zeros(bs, N,device=device) # y\n",
    "        q_traj = np.zeros([T_max, N]) # trajectory\n",
    "        p_traj = np.zeros([T_max, N]) # trajectory\n",
    "\n",
    "        p = 0.01*torch.randn(bs, N,device=device) # random init point\n",
    "\n",
    "        t = 0.0\n",
    "\n",
    "        for i in range(itr):\n",
    "            k = i % max_itr\n",
    "            t = t + self.eps[k]\n",
    "            q, p = self.dSB(k,q,p,t,self.eps[k],J,h,D_SB,pump_SB,xi_SB)\n",
    "            q_traj[i]=q[0,:].cpu().detach().numpy()\n",
    "            p_traj[i]=p[0,:].cpu().detach().numpy()\n",
    "\n",
    "        return q, p, q_traj, p_traj\n",
    "\n",
    "def train(snr):\n",
    "    #SNR\n",
    "    sigma_std = est_SNR(snr, m,n)\n",
    "\n",
    "    network = DU_dSB().to(device)  # generating an instance of TPG-detector\n",
    "    opt = optim.Adam(network.parameters(), lr=lr_adam )  # setting for optimizer\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    torch.manual_seed(1)\n",
    "    network.train()\n",
    "\n",
    "\n",
    "    print(\"-------------------\")\n",
    "    print(\"snr=\",snr)\n",
    "    for i in range(num_batch):#num_batch):\n",
    "        H = H_gen(m,n)\n",
    "        sol = x_gen(bs,N)\n",
    "        y = y_gen(bs,M,sol,H,sigma_std)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        x_hat,_,q_traj ,_= network(H,y,max_itr,bs)\n",
    "\n",
    "        loss = F.mse_loss(x_hat, sol) #squared_loss\n",
    "        loss.backward()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('loss{0}:{1}'.format(i,loss.data), \"BER:\", BER(sol,x_hat.sign())) #print_loss\n",
    "        opt.step()\n",
    "\n",
    "    print(\"-----\\n SNR:\",snr.item())\n",
    "    print(\"eps=\",network.eps)\n",
    "    print(\"beta=\",network.beta)\n",
    "    print(\"xi=\",network.xi)\n",
    "    print(\"lam=\",network.lam**2)\n",
    "    print(\"train done\")\n",
    "    return network\n",
    "\n",
    "# Generalization Error Evaluation\n",
    "def eval(network,snr):\n",
    "    ber_= 0.0\n",
    "    it = max_itr\n",
    "    sigma_std = est_SNR(snr, m,n)\n",
    "\n",
    "    for i in range(sample):\n",
    "        H = H_gen(m,n)\n",
    "        sol = x_gen(bs_,N)\n",
    "        y = y_gen(bs_,M,sol,H,sigma_std)\n",
    "        xx = torch.zeros(bs_,N,device=device)\n",
    "\n",
    "        res = 100*torch.ones(bs_,N,device=device)\n",
    "        x_hat,_,q_traj ,_= network(H,y, it,bs_)\n",
    "        res_ = (y-x_hat.sign()@H).norm(dim=1).view(bs_,1).repeat(1,N).view(bs_,N) # OK\n",
    "        xx[res_<res] = x_hat[res_<res]\n",
    "        res[res_<res] = res_[res_<res]\n",
    "        ber_ += BER(sol,xx.sign())\n",
    "\n",
    "    ber_ = ber_/sample\n",
    "    print(\"SNR:\",snr.item(),\"BER (generalization):\",ber_.item())\n",
    "    return ber_.item()\n",
    "\n",
    "\n",
    "# main part\n",
    "print(\"#_ \", \"n=\", n, \"m=\",m,\"max_itr=\", max_itr, \"bs=\",bs, \"num_batch=\", num_batch,\"learning_rate=\", lr_adam)\n",
    "\n",
    "snr_values = torch.arange(5, 31, 5)\n",
    "ber_results = []\n",
    "\n",
    "for snr in snr_values:\n",
    "    net = train(snr)\n",
    "    ber = eval(net, snr)\n",
    "    ber_results.append(ber)\n",
    "\n",
    "# # 使用 matplotlib 绘制折线图\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.title('DU-LM-SB')\n",
    "# plt.xlabel('SNR (dB)')\n",
    "# plt.ylabel('BER')\n",
    "# plt.ylim(1e-6, 1)\n",
    "# plt.legend()\n",
    "# plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "pytorch2_0_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
